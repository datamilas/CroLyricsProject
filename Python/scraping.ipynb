{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import base64\n",
    "import os\n",
    "import time\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import discogs_client\n",
    "\n",
    "from utils import lower_and_remove_diacritics, check_row_similariy, similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tekstovinet(artist, diskografija_artist_name, url_teksovinet):\n",
    "    # Scraping tekstovi.net for song lyrics by artist (change url to change artist)\n",
    "\n",
    "    ## Create empty df:\n",
    "    column_names = [\"Song_ID\", \"Artist\", \"Artist_diskografija\", \"Song_tekstovinet\", \"Views_tekstovinet\", \"Lyrics_tekstovinet\", \"Url_tekstovinet\"]\n",
    "    df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "\n",
    "    # generate random artist ID and make sure it doesnt have '_'\n",
    "    artist_ID = base64.b64encode(os.urandom(6)).decode('ascii')\n",
    "    while(artist_ID.find('_')>=0):\n",
    "        artist_ID = base64.b64encode(os.urandom(6)).decode('ascii')\n",
    "        \n",
    "    delay = 2\n",
    "\n",
    "    response = get(url_teksovinet)\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    songs = html_soup.find_all('p', class_='artLyrList')\n",
    "\n",
    "\n",
    "    for index, song in enumerate(songs):\n",
    "        song_name = song.findChild(\"a\").text\n",
    "        song_url = song.findChild(\"a\")[\"href\"]\n",
    "\n",
    "        url = 'https://tekstovi.net/'+song_url\n",
    "\n",
    "        response = get(url)\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        song_lyrics = html_soup.find_all('p', class_='lyric')\n",
    "        lyrics = [l.text for l in song_lyrics]\n",
    "        lyrics = \" \".join(lyrics)\n",
    "        song_ID = artist_ID + '_' + str(1) + '_' + str(index)\n",
    "        views_text = html_soup.find('p', class_='lyric_impressions').text\n",
    "        views_count = int(\"\".join([c for c in views_text if c.isdigit()]))\n",
    "        new_entry = pd.DataFrame([[song_ID, artist, diskografija_artist_name, song_name, views_count, lyrics, url]], columns=[\"Song_ID\", \"Artist\", \"Artist_diskografija\", \"Song_tekstovinet\", \"Views_tekstovinet\", \"Lyrics_tekstovinet\", \"Url_tekstovinet\"])\n",
    "        df = df.append(new_entry, ignore_index=True)\n",
    "\n",
    "        print(\"Lyrics successfully written to file for : \" + song_name)\n",
    "                            \n",
    "        time.sleep(delay)\n",
    "\n",
    "    df.to_csv(f'data/{artist}_tekstovinet.csv', index=False)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_google(search_query):\n",
    "     #search\n",
    "    driver.find_element(By.NAME, 'q').clear()\n",
    "    search_box = driver.find_element(By.NAME, 'q')\n",
    "    search_box.send_keys(search_query)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyricstranslate():\n",
    "\n",
    "    driver.find_element(By.PARTIAL_LINK_TEXT, 'lyricstranslate.com').send_keys(Keys.CONTROL + Keys.RETURN)\n",
    "    time.sleep(0.5)\n",
    "    window_handles = driver.window_handles\n",
    "    driver.switch_to.window(window_name=window_handles[1])    \n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "    try: driver.find_element(By.XPATH, \"//*[contains(text(), 'AGREE')]\").click()\n",
    "    except: pass\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    try: driver.find_element(By.PARTIAL_LINK_TEXT, 'English').click()\n",
    "    except: pass\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    try: driver.find_element(By.PARTIAL_LINK_TEXT, 'Click to see the original lyrics').click()\n",
    "    except: pass\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        original_lyrics_raw = driver.find_element(By.CLASS_NAME, 'song-node-text')\n",
    "        original_title = original_lyrics_raw.find_element(By.CLASS_NAME, 'title-h2').text\n",
    "        par_class = original_lyrics_raw.find_elements(By.CLASS_NAME, 'par')\n",
    "        original_lyrics = \". \".join([t.text for t in par_class])\n",
    "        original_lyrics = original_lyrics.replace(\"*\", \"\")\n",
    "    except: \n",
    "        print(\"Couldn't find english lyrics\")\n",
    "        original_title = np.nan\n",
    "        original_lyrics = np.nan\n",
    "\n",
    "    try:\n",
    "        eng_lyrics_raw = driver.find_element(By.CLASS_NAME, 'translate-node-text')\n",
    "        eng_title = eng_lyrics_raw.find_element(By.CLASS_NAME, 'title-h2').text\n",
    "        par_class = eng_lyrics_raw.find_elements(By.CLASS_NAME, 'par')\n",
    "        eng_lyrics = \". \".join([t.text for t in par_class])\n",
    "        eng_lyrics = eng_lyrics.replace(\"*\", \"\")\n",
    "    except: \n",
    "        print(\"Couldn't find english lyrics\")\n",
    "        eng_title = np.nan\n",
    "        eng_lyrics = np.nan\n",
    "\n",
    "        \n",
    "    try:\n",
    "        copyright = original_lyrics_raw.find_element(By.CLASS_NAME, 'copyrighttext').text.split(\"\\n\")\n",
    "        writer_info = [c for c in copyright if \"Writer\" in c][0]\n",
    "        if len(writer_info)>0:\n",
    "            writer = writer_info.split(\": \")[1]\n",
    "\n",
    "    except: \n",
    "        print(\"Couldn't find writer info\")\n",
    "        writer = np.nan\n",
    "\n",
    "\n",
    "    try:\n",
    "        song_info = driver.find_element(By.CLASS_NAME, 'song-node-info')\n",
    "        album_name = song_info.find_element(By.XPATH, \"//*[contains(text(), 'Album')]\").text\n",
    "        album_name = album_name.split(\": \")[1]\n",
    "    except: \n",
    "        print(\"Couldn't find album info\")\n",
    "        album_name = np.nan\n",
    "\n",
    "    \n",
    "    song_url = driver.current_url\n",
    "    driver.close()\n",
    "    driver.switch_to.window(window_name=window_handles[0])\n",
    "    \n",
    "    \n",
    "    return writer, original_title, eng_title, album_name, original_lyrics, eng_lyrics, song_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_lyricstranslate(artist_name):\n",
    "    df = pd.read_csv(f\"data/{artist_name}_tekstovinet.csv\")\n",
    "\n",
    "    ser = Service(\"chromedriver.exe\")\n",
    "    op = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=ser, options=op)\n",
    "    driver.get(\"https://google.com\")\n",
    "    time.sleep(0.5)\n",
    "    try: driver.find_element(By.XPATH, \"//*[contains(text(), 'Aceito')]\").click()\n",
    "    except: driver.find_element(By.XPATH, \"//*[contains(text(), 'I agree')]\").click()\n",
    "    time.sleep(0.5)\n",
    "    try: driver.find_element(By.XPATH, \"//*[contains(text(), 'English')]\").click()\n",
    "    except: pass\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    df_new = df.copy()#[0:1]##[16:17]\n",
    "    for row in df_new.iterrows():\n",
    "        time.sleep(1)\n",
    "        artist_name = row[1].Artist\n",
    "        song_name = row[1].Song_tekstovinet\n",
    "        try: song_name = song_name[0:song_name.index(\"(\")]\n",
    "        except:pass\n",
    "        \n",
    "        try:\n",
    "            search_google(artist_name+\" \"+song_name+ \" english lyricstranslate\")\n",
    "            writer, original_title, eng_title, album_name, original_lyrics, eng_lyrics, song_url = get_lyricstranslate()\n",
    "\n",
    "            df.loc[row[0], \"Writer_lyricstranslate\"] = writer \n",
    "            df.loc[row[0], \"Song_lyricstranslate\"] = original_title\n",
    "            df.loc[row[0], \"EngTitle_lyricstranslate\"] = eng_title\n",
    "            df.loc[row[0], \"Album_lyricstranslate\"] = album_name\n",
    "            df.loc[row[0], \"Lyrics_lyricstranslate\"] = original_lyrics\n",
    "            df.loc[row[0], \"EngLyrics_lyricstranslate\"] = eng_lyrics\n",
    "            df.loc[row[0], \"Url_lyricstranslate\"] = song_url\n",
    "\n",
    "            print(f\"Succesfully found information for {song_name} from {artist_name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Problem with finding info for {song_name} from {artist_name}\")\n",
    "            print(e)\n",
    "        print(\"#######\")\n",
    "        time.sleep(3)\n",
    "\n",
    "    wrong_rows = check_row_similariy(df, \"Lyrics_tekstovinet\", \"Lyrics_lyricstranslate\", threshold=0.7)\n",
    "    \n",
    "    ##set parameters as nan for wrong rows\n",
    "    for row in wrong_rows:\n",
    "        df.loc[row, \"Writer_lyricstranslate\"] = np.nan\n",
    "        df.loc[row, \"Song_lyricstranslate\"] = np.nan\n",
    "        df.loc[row, \"EngTitle_lyricstranslate\"] = np.nan\n",
    "        df.loc[row, \"Album_lyricstranslate\"] = np.nan\n",
    "        df.loc[row, \"Lyrics_lyricstranslate\"] = np.nan\n",
    "        df.loc[row, \"EngLyrics_lyricstranslate\"] = np.nan\n",
    "        df.loc[row, \"Url_lyricstranslate\"] = np.nan\n",
    "\n",
    "    df.to_csv(f\"data/{artist_name}_tekstovinet_lyricstranslate.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_for_authors_and_albums(song_name, artist_name):\n",
    "    #search\n",
    "    driver.find_element(By.CLASS_NAME, 'search-page').clear()\n",
    "    search_box = driver.find_element(By.CLASS_NAME, 'search-page')\n",
    "    song_name = lower_and_remove_diacritics(song_name)\n",
    "    search_box.send_keys(song_name)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    #get results of search\n",
    "    #search_result = driver.find_element(By.CLASS_NAME, 'search-result')\n",
    "    try:\n",
    "        song_results = driver.find_element(By.XPATH, f\"//*[contains(text(), 'Pjesme')]/following-sibling::ul\")\n",
    "        song_results = song_results.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "        #go through search results\n",
    "        for song in song_results:\n",
    "\n",
    "            song.send_keys(Keys.CONTROL + Keys.RETURN)\n",
    "            window_handles = driver.window_handles\n",
    "\n",
    "            driver.switch_to.window(driver.window_handles[1])\n",
    "            time.sleep(1)            \n",
    "\n",
    "            if  check_performer(artist_name):\n",
    "                subfields, authors = get_authors()\n",
    "                albums = get_albums()\n",
    "                song_url = driver.current_url\n",
    "                song_name_website = driver.find_element(By.XPATH, f\"//*[contains(text(), 'Pjesma')]/following-sibling::h1\").text\n",
    "                driver.close()\n",
    "                driver.switch_to.window(window_name=window_handles[0])\n",
    "                return subfields, authors, albums, song_url, song_name_website\n",
    "                \n",
    "            else:\n",
    "                driver.close()\n",
    "                driver.switch_to.window(window_name=window_handles[0])\n",
    "    \n",
    "    except Exception as e: print(f\"Problem with scraping authors and albums for song {song_name}\")\n",
    "\n",
    "    return False    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_performer(artist_name):\n",
    "    authors = driver.find_elements(By.TAG_NAME, \"h3\")[1:]\n",
    "    artist_name = lower_and_remove_diacritics(artist_name)\n",
    "    for index, author in enumerate(authors):\n",
    "        subfield = author.text.capitalize()\n",
    "        if subfield == \"Izvedba\":\n",
    "            siblings_of_current = driver.find_elements(By.XPATH, f\"//*[contains(text(), '{subfield}')]/following-sibling::a\")\n",
    "            if index<len(authors)-1: \n",
    "                siblings_of_next = driver.find_elements(By.XPATH, f\"//*[contains(text(), '{authors[index+1].text.capitalize()}')]/following-sibling::a\")\n",
    "                result = [r.text for r in siblings_of_current[0: len(siblings_of_current)-len(siblings_of_next)]]\n",
    "            else: result = [r.text for r in siblings_of_current] \n",
    "            \n",
    "    return True if len([r for r in result if artist_name in lower_and_remove_diacritics(r)])>0 else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors():\n",
    "    all_fields = []\n",
    "    all_authors = []\n",
    "    #authors = driver.find_elements(By.TAG_NAME, \"h3\")[1:]\n",
    "    authors = driver.find_elements(By.XPATH, f\"//*[contains(text(), 'Autori')]/following-sibling::h3\")\n",
    "    for index, author in enumerate(authors):\n",
    "        subfield = author.text.capitalize()\n",
    "        siblings_of_current = driver.find_elements(By.XPATH, f\"//*[contains(text(), '{subfield}')]/following-sibling::a\")\n",
    "        if len(siblings_of_current)>0:\n",
    "            if index<len(authors)-1: \n",
    "                siblings_of_next = driver.find_elements(By.XPATH, f\"//*[contains(text(), '{authors[index+1].text.capitalize()}')]/following-sibling::a\")\n",
    "                result = [r.text for r in siblings_of_current[0: len(siblings_of_current)-len(siblings_of_next)]]\n",
    "            else: result = [r.text for r in siblings_of_current]\n",
    "            all_fields.append(subfield)\n",
    "            all_authors.append(str(result))\n",
    "            \n",
    "    return all_fields, all_authors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_albums():\n",
    "    albums = driver.find_elements(By.XPATH, f\"//*[contains(text(), 'Albumi')]/following-sibling::div\")\n",
    "    return [{album.text} for album in albums]      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_diskografija(artist_name):\n",
    "    df = pd.read_csv(f\"data/{artist_name}_tekstovinet_lyricstranslate.csv\")\n",
    "    \n",
    "    ser = Service(\"chromedriver.exe\")\n",
    "    op = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=ser, options=op)\n",
    "    driver.get(\"https://diskografija.com/\")\n",
    "\n",
    "\n",
    "    df_new = df.copy()#[0:5]\n",
    "    for row in df_new.iterrows():\n",
    "        artist_name = row[1].Artist_diskografija\n",
    "        song_name = row[1].Song_lyricstranslate\n",
    "        if type(song_name)==float: song_name = row[1].Song_tekstovinet\n",
    "        try: song_name = song_name[0:song_name.index(\"(\")]\n",
    "        except:pass\n",
    "        \n",
    "        try: \n",
    "            subfields, authors, albums, song_url, song_title_website = scrape_for_authors_and_albums(song_name, artist_name)\n",
    "        \n",
    "            for subfield, author in zip(subfields, authors):\n",
    "                df.loc[row[0], f\"{subfield}_diskografija\"] = author\n",
    "                \n",
    "            for i, album in enumerate(albums):\n",
    "                df.loc[row[0], f\"Album_{i+1}_diskografija\"] = album\n",
    "            df.loc[row[0], \"Url_diskografija\"] = song_url\n",
    "            df.loc[row[0], \"Song_diskografija\"] = song_title_website\n",
    "\n",
    "            print(f\"Succesfully found information for {song_name} from {artist_name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Problem with finding info for {song_name} from {artist_name}\")\n",
    "            print(e)\n",
    "        print(\"#######\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "\n",
    "    df.to_csv(f\"data/{artist_name}_tekstovinet_lyricstranslate_diskografija.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cuspajz(artist_name, url_cuspajz):\n",
    "    df = pd.read_csv(f\"data/{artist_name}_tekstovinet_lyricstranslate_diskografija.csv\")\n",
    "\n",
    "    # Scraping cuspajz.com for song lyrics by artist\n",
    "\n",
    "    response = get(url_cuspajz)\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Get the list of songs\n",
    "    songs = html_soup.find_all('ul', class_='songList')\n",
    "\n",
    "    song_names = []\n",
    "    song_urls = []\n",
    "    for col_index, column in enumerate(songs):\n",
    "        songs = column.find_all('li')\n",
    "        for index, song in enumerate(songs):\n",
    "            song_names.append(song.a.text)\n",
    "            song_urls.append(song.a['href'])\n",
    "\n",
    "\n",
    "    similarity_threshold = 0.9\n",
    "    for row in df.iterrows():\n",
    "        song_title = lower_and_remove_diacritics(row[1].Song_tekstovinet).split('(')[0]\n",
    "        similarities = [similar(song_title, lower_and_remove_diacritics(s.split('(')[0])) for s in song_names]\n",
    "        max_similarity = max(similarities)\n",
    "        \n",
    "        if max_similarity>similarity_threshold:\n",
    "            cuspajz_name = song_names[similarities.index(max_similarity)] \n",
    "            song_url = song_urls[similarities.index(max_similarity)]\n",
    "            url = 'https://cuspajz.com/'+song_url\n",
    "\n",
    "            response = get(url)\n",
    "            html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            song_lyrics = html_soup.find_all('p', class_='clearfix')\n",
    "            lyrics =  song_lyrics[0].get_text()\n",
    "            \n",
    "            df.loc[row[0], \"Lyrics_cuspajz\"] = lyrics\n",
    "            df.loc[row[0], \"Song_cuspajz\"] = cuspajz_name\n",
    "            df.loc[row[0], \"Url_cuspajz\"] = url\n",
    "            \n",
    "\n",
    "            print(\"Lyrics successfully written to file for : \" + song_title)\n",
    "                        \n",
    "            time.sleep(3)\n",
    "            \n",
    "\n",
    "    wrong_rows = check_row_similariy(df, \"Lyrics_tekstovinet\", \"Lyrics_cuspajz\", threshold=0.7)\n",
    "    ##set parameters as nan for wrong rows\n",
    "    for row in wrong_rows:\n",
    "        df.loc[row[0], \"Lyrics_cuspajz\"] = np.nan\n",
    "        df.loc[row[0], \"Song_cuspajz\"] = np.nan\n",
    "        df.loc[row[0], \"Url_cuspajz\"] = np.nan\n",
    "\n",
    "    df.to_csv(f'data/{artist_name}_tekstovinet_lyricstranslate_diskografija_cuspajz.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_discogs(artist_name, artist_id_discogs):\n",
    "    d = discogs_client.Client('datamilas/0.1', user_token='BDGKHOLMZJxywoowKXvyRhXeqJhkezYxwVjxvBJR')\n",
    "    artist_object = d.artist(artist_id_discogs)\n",
    "    artist_discogs = artist_object.name\n",
    "    albums = artist_object.releases\n",
    "    df_discogs = pd.DataFrame(columns=[\"Artist\"])\n",
    "\n",
    "\n",
    "    i=0\n",
    "    for album in albums:\n",
    "        if album.data[\"role\"] == \"Main\":\n",
    "            print(album)\n",
    "            time.sleep(0.5)\n",
    "            for track in album.tracklist:\n",
    "                if len(track.artists)==0 or (artist_object in track.artists):\n",
    "                    df_discogs.loc[i, \"Artist\"] = artist_name\n",
    "                    df_discogs.loc[i, \"Artist_discogs\"] = artist_discogs\n",
    "                    df_discogs.loc[i, \"Song_discogs\"] = track.title\n",
    "                    if \"duration\" in track.data.keys(): df_discogs.loc[i, \"Song_duration_discogs\"] = track.data[\"duration\"]\n",
    "                    if \"extraartists\" in track.data.keys():\n",
    "                        for ea in track.data[\"extraartists\"]:\n",
    "                            roles = ea[\"role\"].split(\", \")\n",
    "\n",
    "                            for role in roles:\n",
    "                                try: df_discogs.loc[i, f\"{role}_discogs\"] = df_discogs.loc[i, role]+\"/\"+ea[\"name\"]\n",
    "                                except: df_discogs.loc[i, f\"{role}_discogs\"] = ea[\"name\"]\n",
    "                    df_discogs.loc[i, \"Album_title_discogs\"] = album.title\n",
    "                    df_discogs.loc[i, \"Album_year_discogs\"] = album.year\n",
    "                    df_discogs.loc[i, \"Album_genres_discogs\"] = str(album.genres)\n",
    "\n",
    "                    i+=1\n",
    "\n",
    "\n",
    "    df_discogs[\"Album_year_discogs\"] = df_discogs[\"Album_year_discogs\"].astype(int)\n",
    "    df_discogs[\"Album_year_title_discogs\"] = df_discogs[\"Album_year_discogs\"].astype(str)+\"-\"+df_discogs[\"Album_title_discogs\"]\n",
    "    df_discogs[\"All_albums_discogs\"] = df_discogs.groupby(\"Song_discogs\")[\"Album_year_title_discogs\"].transform(lambda x: '/'.join(x))\n",
    "\n",
    "    df_discogs.sort_values(\"Album_year_discogs\").reset_index(drop=True, inplace=True)\n",
    "    df_discogs = df_discogs.drop_duplicates(subset=[\"Song_discogs\"])\n",
    "    df_discogs = df_discogs.reset_index(drop=True)\n",
    "\n",
    "    columns_to_take = [col for col in df_discogs.columns if col in ['Artist_discogs', 'Song_discogs', 'Song_duration_discogs',\n",
    "       'Album_title_discogs', 'Album_year_discogs', 'Album_genres_discogs','All_albums_discogs',\n",
    "       'Lyrics By_discogs', 'Music By_discogs', 'Featuring_discogs']]\n",
    "\n",
    "    df = pd.read_csv(f\"data/{artist_name}_tekstovinet_lyricstranslate_diskografija_cuspajz.csv\")\n",
    "\n",
    "    similarity_threshold = 0.9\n",
    "    song_names = [lower_and_remove_diacritics(s).split('(')[0] for s in df_discogs.Song_discogs]\n",
    "    for row in df.iterrows():\n",
    "        song_title = lower_and_remove_diacritics(row[1].Song_tekstovinet).split('(')[0]\n",
    "        similarities = [similar(song_title, s) for s in song_names]\n",
    "        max_similarity = max(similarities)\n",
    "        \n",
    "        if max_similarity>similarity_threshold:\n",
    "            df.loc[row[0], columns_to_take] = df_discogs.loc[similarities.index(max_similarity)][columns_to_take]\n",
    "\n",
    "    df.to_csv(f\"data/{artist_name}_final.csv\", index=False)\n",
    "    df_discogs.to_csv(f\"data/{artist_name}_discogs.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_name = \"Magazin\"\n",
    "artist_name_diskografija = \"Magazin\"\n",
    "url_tekstovinet = \"https://tekstovi.net/2,95,0.html\"\n",
    "url_cuspajz = \"https://cuspajz.com/tekstovi-pjesama/izvodjac/magazin.html\"\n",
    "artist_id_discogs = 345842\n",
    "scrape_tekstovinet(artist_name, artist_name_diskografija, url_tekstovinet)\n",
    "scrape_lyricstranslate(artist_name)\n",
    "scrape_diskografija(artist_name)\n",
    "scrape_cuspajz(artist_name, url_cuspajz)\n",
    "scrape_discogs(artist_name, artist_id_discogs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e404b59586357c814bc0d3940e75d6763c00a48753b225b81f7716971b8e1741"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
