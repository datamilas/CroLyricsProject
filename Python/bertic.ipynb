{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"classla/bcms-bertic\")\n",
    "#token_classification = AutoModelForTokenClassification.from_pretrained(\"classla/bcms-bertic\")\n",
    "\n",
    "\n",
    "model = AutoModelForPreTraining.from_pretrained(\"classla/bcms-bertic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.custom_models.models import ElectraForMultiLabelSequenceClassification\n",
    "model = ElectraForMultiLabelSequenceClassification.from_pretrained(\"classla/bcms-bertic-geo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ElectraForMultiLabelSequenceClassification' has no attribute 'load_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-120cf768da4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mElectraForMultiLabelSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"classla/bcms-bertic-geo\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'ElectraForMultiLabelSequenceClassification' has no attribute 'load_model'"
     ]
    }
   ],
   "source": [
    "ElectraForMultiLabelSequenceClassification.load_model(\"classla/bcms-bertic-geo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'loss_fct'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-11ae230cc480>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Create a ClassificationModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m model = MultiLabelClassificationModel(\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;34m\"electra\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;34m\"classla/bcms-bertic-geo\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python38\\lib\\site-packages\\simpletransformers\\classification\\multi_label_classification_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_type, model_name, num_labels, pos_weight, args, use_cuda, cuda_device, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m                 )\n\u001b[0;32m    227\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                 self.model = model_class.from_pretrained(\n\u001b[0m\u001b[0;32m    229\u001b[0m                     \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                 )\n",
      "\u001b[1;32mC:\\Python\\Python38\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1487\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mno_init_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_enable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_fast_init\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1489\u001b[1;33m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfrom_pt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'loss_fct'"
     ]
    }
   ],
   "source": [
    "#https://github.com/clarinsi/geobert\n",
    "\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class GlobalScaler():\n",
    "\tdef __init__(self):\n",
    "\t\tself.means = None\n",
    "\t\tself.stddev = None\n",
    "\n",
    "\tdef fit_transform(self, data):\n",
    "\t\tself.means = np.mean(data, axis=0)\n",
    "\t\tcentereddata = data - self.means\n",
    "\t\tself.stddev = np.std(centereddata)\n",
    "\t\treturn centereddata / self.stddev\n",
    "\n",
    "\tdef transform(self, data):\n",
    "\t\treturn (data - self.means) / self.stddev\n",
    "\n",
    "\tdef inverse_transform(self, data):\n",
    "\t\treturn (np.asarray(data) * self.stddev) + self.means\n",
    "\n",
    "scl=pickle.load(open('bcms.scaler','rb'))\n",
    "\n",
    "# Setting optional model configuration\n",
    "model_args = {\n",
    "    \"regression\": True,\n",
    "    \"do_lower_case\": True,\n",
    "    \"eval_batch_size\": 64,\n",
    "\t\"loss_fct\": \"MAELoss\",\n",
    "}\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = MultiLabelClassificationModel(\n",
    "    \"electra\",\n",
    "    \"classla/bcms-bertic-geo\",\n",
    "    num_labels=2,\n",
    "    args=model_args,\n",
    "\tloss_fct=\"MAELoss\",\n",
    "\tuse_cuda=False\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42.37963124, 19.2159317] {'country_code': 'ME', 'city': 'Mojanovići', 'country': 'Montenegro'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import reverse_geocode\n",
    "\n",
    "pred_inv= [[42.37963124, 19.2159317 ]]\n",
    "\n",
    "\n",
    "pred_rev = reverse_geocode.search(pred_inv)\n",
    "for c, r in zip(pred_inv, pred_rev):\n",
    "\tprint(c,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define which data to analyze\n",
    "\n",
    "#by song_ID\n",
    "df[\"toAnalyze\"] = df[\"Song_ID\"].str.find('oOIOCzZV') \n",
    "#df_old = df[df.toAnalyze == -1]\n",
    "#df_old = df_old.drop([\"toAnalyze\"], axis=1)\n",
    "\n",
    "df = df[df.toAnalyze >= 0]\n",
    "df = df.reset_index()\n",
    "df = df.drop([\"index\", \"toAnalyze\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tagged = pd.DataFrame()\n",
    "\n",
    "df_tagged_lyrics = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-dfca5d082158>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msong\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msongs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdf_tagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_tagged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "songs = df[\"Song\"]\n",
    "lyrics = df[\"Lyrics\"]\n",
    "\n",
    "for index, song in enumerate(songs):\n",
    "    tagged = json.loads(n.tag(song))\n",
    "    df_tagged = df_tagged.append(tagged, ignore_index=True)\n",
    "    \n",
    "    text = lyrics[index]\n",
    "    tagged_text = json.loads(n.tag(text))\n",
    "    df_tagged_lyrics = df_tagged_lyrics.append(tagged_text, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs=[l.splitlines() for l in lyrics.dropna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_lyrics = [\" \".join(l) for l in raw_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[51,\n",
       "  'svi dani, jutra i noći prazni su kao moj stan  imati nekog kraj sebe  to je moj san, moj davni san  pitanje kojim ko dijete  majci ja otkrih svoj jad još uvijek u srcu mi živi gori sad, boli sad  kaži mi kaži mama  zašto sam ja sama a Zlatka sa prvog kata  ima sestru i dva brata  pitanje kojim o dijete  majci ja otkrih svoj jad  još uvijek u srcu i živi  gori sad, boli sad  kaži mi kaži mama... još uvijek u srcu mi živi'],\n",
       " [58,\n",
       "  'Iza vitra šta u provu tuče iza mora šta me na dno vuče vrate mene moji maestrali U moj porat, u moj porat mali U moj porat, u moj porat mali  Sidin opet kraj starog komina Napijen se domaćega vina Pa mi mater posteju namisti Lancuni su od ditinjstva isti Lancuni su od ditinjstva isti  Ne budi me mati Ne budi , ne zovi Sin tvoj na kušinu u ditinjstvo plovi U ditinjstvo plovi Ne budi me mati Kad zora dođe Sve grubo i teško neka u sne prođe Neka u sne prođe Ne budi me mati  Kukuriče kokot iza kuće Penšjunati igraju na buće Nestalo je vrimena i ura Dok ja sanjan iza starih škura Dok ja sanjan iza starih škura  Ne budi me mati Ne budi ne zovi Sin tvoj na kušinu u ditinjstvo plovi U ditinjstvo plovi U ditinjstvo plovi Ne budi me mati Kad zora dođe Sve grubo i teško neka u snove prođe Neka u sne prođe Ne budi me mati.']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[i, song] for i, song in enumerate(song_lyrics) if \"mati\" in song]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prijatelji dolaze na vrijeme I vec su upaljene svijece Darovi se dijele Prava slika srece  I noc se ova Silverstarska trosi Uz poljupce i kolace I mnogo lijepih zelja A mozda se nekom place  Kad odu svi Kad ostanemo sami U nasoj sobi Sami ja i ti  Kad odu svi U praznickoj tami Bez nade i snova Ostat cemo mi  I ti ces reci Ti ces samo hladno raci Sretna Nova Sretna Nova  Dimnjacar zadnji ide niz cestu Jos se piju razna pica Jos minuta i ponoc tuce Nasoj ljubavi bez pokrica'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_lyrics[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iza vitra šta u provu tuče iza mora šta me na dno vuče vrate mene moji maestrali U moj porat, u moj porat mali U moj porat, u moj porat mali  Sidin opet kraj starog komina Napijen se domaćega vina Pa mi mater posteju namisti Lancuni su od ditinjstva isti Lancuni su od ditinjstva isti  Ne budi me mati Ne budi , ne zovi Sin tvoj na kušinu u ditinjstvo plovi U ditinjstvo plovi Ne budi me mati Kad zora dođe Sve grubo i teško neka u sne prođe Neka u sne prođe Ne budi me mati  Kukuriče kokot iza kuće Penšjunati igraju na buće Nestalo je vrimena i ura Dok ja sanjan iza starih škura Dok ja sanjan iza starih škura  Ne budi me mati Ne budi ne zovi Sin tvoj na kušinu u ditinjstvo plovi U ditinjstvo plovi U ditinjstvo plovi Ne budi me mati Kad zora dođe Sve grubo i teško neka u snove prođe Neka u sne prođe Ne budi me mati.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_lyrics[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_lyrics.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(song_lyrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_lyrics.txt', 'r', encoding=\"utf-8\") as file:\n",
    "    data = file.read().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iza vitra šta u provu tuče iza mora šta me na dno vuče vrate mene moji maestrali U moj porat, u moj porat mali U moj porat, u moj porat mali  Sidin opet kraj starog komina Napijen se domaćega vina Pa mi mater posteju namisti Lancuni su od ditinjstva isti Lancuni su od ditinjstva isti  Ne budi me mati Ne budi , ne zovi Sin tvoj na kušinu u ditinjstvo plovi U ditinjstvo plovi Ne budi me mati Kad zora dođe Sve grubo i teško neka u sne prođe Neka u sne prođe Ne budi me mati  Kukuriče kokot iza kuće Penšjunati igraju na buće Nestalo je vrimena i ura Dok ja sanjan iza starih škura Dok ja sanjan iza starih škura  Ne budi me mati Ne budi ne zovi Sin tvoj na kušinu u ditinjstvo plovi U ditinjstvo plovi U ditinjstvo plovi Ne budi me mati Kad zora dođe Sve grubo i teško neka u snove prođe Neka u sne prođe Ne budi me mati.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = \"Mrzim ovo sve aaaaaaa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(model_inputs, return_tensors=\"pt\")\n",
    "labels = torch.tensor([[1, 1]], dtype=torch.float)  # need dtype=float for BCEWithLogitsLoss\n",
    "outputs = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"classla/bcms-bertic-ner\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"classla/bcms-bertic-ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "ners = nlp_token_class(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': 0.78599304,\n",
       "  'word': 'moše pijade',\n",
       "  'start': 371,\n",
       "  'end': 382}]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.9925545,\n",
       "  'index': 3,\n",
       "  'word': 'Ana',\n",
       "  'start': 7,\n",
       "  'end': 10},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.9909462,\n",
       "  'index': 5,\n",
       "  'word': 'Zagreba',\n",
       "  'start': 14,\n",
       "  'end': 21}]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "p(\"Ja sam Ana iz Zagreba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at classla/bcms-bertic were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The model 'ElectraForPreTraining' is not supported for . Supported models are ['YosoForTokenClassification', 'NystromformerForTokenClassification', 'QDQBertForTokenClassification', 'FNetForTokenClassification', 'LayoutLMv2ForTokenClassification', 'RemBertForTokenClassification', 'CanineForTokenClassification', 'RoFormerForTokenClassification', 'BigBirdForTokenClassification', 'ConvBertForTokenClassification', 'LayoutLMForTokenClassification', 'DistilBertForTokenClassification', 'CamembertForTokenClassification', 'FlaubertForTokenClassification', 'XLMForTokenClassification', 'XLMRobertaForTokenClassification', 'LongformerForTokenClassification', 'RobertaForTokenClassification', 'SqueezeBertForTokenClassification', 'BertForTokenClassification', 'MegatronBertForTokenClassification', 'MobileBertForTokenClassification', 'XLNetForTokenClassification', 'AlbertForTokenClassification', 'ElectraForTokenClassification', 'FunnelForTokenClassification', 'MPNetForTokenClassification', 'DebertaForTokenClassification', 'DebertaV2ForTokenClassification', 'GPT2ForTokenClassification', 'IBertForTokenClassification'].\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-215-6c9171c18c2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtoken_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"classla/bcms-bertic\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenClassificationPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'My name is Clara and I live in Berkeley, California.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python\\Python38\\lib\\site-packages\\transformers\\pipelines\\token_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"offset_mapping\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moffset_mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python38\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1004\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1006\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python38\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36mrun_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0mmodel_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python38\\lib\\site-packages\\transformers\\pipelines\\token_classification.py\u001b[0m in \u001b[0;36mpostprocess\u001b[1;34m(self, model_outputs, aggregation_strategy, ignore_labels)\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset_mapping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_tokens_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregation_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         )\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[0mgrouped_entities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_entities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregation_strategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[1;31m# Filter anything that is in self.ignore_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         entities = [\n",
      "\u001b[1;32mC:\\Python\\Python38\\lib\\site-packages\\transformers\\pipelines\\token_classification.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, pre_entities, aggregation_strategy)\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mpre_entity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpre_entities\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[0mentity_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_entity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scores\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                 \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_entity\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scores\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentity_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m                 entity = {\n\u001b[0;32m    321\u001b[0m                     \u001b[1;34m\"entity\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentity_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"classla/bcms-bertic\")\n",
    "model = AutoModelForPreTraining.from_pretrained(\"classla/bcms-bertic\")\n",
    "token_classifier = AutoModelForTokenClassification.from_pretrained(\"classla/bcms-bertic\")\n",
    "p = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "p('My name is Clara and I live in Berkeley, California.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.custom_models.models import ElectraForMultiLabelSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"classla/bcms-bertic-geo\")\n",
    "\n",
    "model = ElectraForMultiLabelSequenceClassification.from_pretrained(\"classla/bcms-bertic-geo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 370/370 [00:00<00:00, 74.0kB/s]\n",
      "Downloading: 100%|██████████| 321k/321k [00:00<00:00, 573kB/s]  \n",
      "Downloading: 100%|██████████| 700k/700k [00:00<00:00, 896kB/s]  \n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 18.7kB/s]\n",
      "Downloading: 100%|██████████| 721/721 [00:00<00:00, 120kB/s]\n",
      "Downloading: 100%|██████████| 474M/474M [02:18<00:00, 3.58MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"classla/bcms-bertic-frenk-hate\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"classla/bcms-bertic-frenk-hate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:21<00:21, 21.38s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0], dtype=int64)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "model_args = {\n",
    "        \"num_train_epochs\": 12,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"train_batch_size\": 74}\n",
    "\n",
    "model = ClassificationModel(\n",
    "    \"bert\", \"classla/bcms-bertic-frenk-hate\", use_cuda=False,\n",
    "    args=model_args\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "### Output:\n",
    "### array([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:16<05:13, 16.50s/it]\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, logit_output = model.predict(model_inputs[0:20])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EMBEDDIA/crosloengual-bert\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"EMBEDDIA/crosloengual-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne govorim s tobom, i ništa nije kao prije, čija to pisma čitaš? tišina, samo tišina lije.  hej, pogledaj me u oči, što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta?  ako se rastanemo, onako pošteno i sami, što ćemo reći susjedima, poštaru i tvojoj mami? ako se rastanemo, neće se srušiti svijet, doći će novi stanari, u moše pijade 35  ne govorim s tobom, drugačija si, nisi ista, izgubili smo nepovratno, djetelinu s četiri lista.  daj, pogledaj me u oči što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta?\n"
     ]
    }
   ],
   "source": [
    "text=model_inputs[0]\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ne',\n",
       " 'govorim',\n",
       " 's',\n",
       " 'tobo',\n",
       " '##m',\n",
       " ',',\n",
       " 'i',\n",
       " 'ništa',\n",
       " 'nije',\n",
       " 'kao',\n",
       " 'prije',\n",
       " ',',\n",
       " 'čija',\n",
       " 'to',\n",
       " 'pisma',\n",
       " 'čita',\n",
       " '##š',\n",
       " '?',\n",
       " 'tišin',\n",
       " '##a',\n",
       " ',',\n",
       " 'samo',\n",
       " 'tišin',\n",
       " '##a',\n",
       " 'lije',\n",
       " '.',\n",
       " 'he',\n",
       " '##j',\n",
       " ',',\n",
       " 'pogleda',\n",
       " '##j',\n",
       " 'me',\n",
       " 'u',\n",
       " 'oči',\n",
       " ',',\n",
       " 'što',\n",
       " 'čovjek',\n",
       " 'može',\n",
       " 'da',\n",
       " 'prog',\n",
       " '##uta',\n",
       " ',',\n",
       " 'ima',\n",
       " 'li',\n",
       " 'snage',\n",
       " 'za',\n",
       " 'još',\n",
       " 'pet',\n",
       " 'minuta',\n",
       " '?',\n",
       " 'ili',\n",
       " 'smo',\n",
       " 'stigli',\n",
       " 'do',\n",
       " 'kraja',\n",
       " ',',\n",
       " 'do',\n",
       " 'kraja',\n",
       " 'puta',\n",
       " '?',\n",
       " 'ako',\n",
       " 'se',\n",
       " 'rasta',\n",
       " '##nemo',\n",
       " ',',\n",
       " 'onako',\n",
       " 'pošteno',\n",
       " 'i',\n",
       " 'sami',\n",
       " ',',\n",
       " 'što',\n",
       " 'ćemo',\n",
       " 'reći',\n",
       " 'susjedi',\n",
       " '##ma',\n",
       " ',',\n",
       " 'pošta',\n",
       " '##ru',\n",
       " 'i',\n",
       " 'tvoj',\n",
       " '##oj',\n",
       " 'mami',\n",
       " '?',\n",
       " 'ako',\n",
       " 'se',\n",
       " 'rasta',\n",
       " '##nemo',\n",
       " ',',\n",
       " 'neće',\n",
       " 'se',\n",
       " 'sruši',\n",
       " '##ti',\n",
       " 'svijet',\n",
       " ',',\n",
       " 'doći',\n",
       " 'će',\n",
       " 'novi',\n",
       " 'stanar',\n",
       " '##i',\n",
       " ',',\n",
       " 'u',\n",
       " 'moš',\n",
       " '##e',\n",
       " 'pij',\n",
       " '##ade',\n",
       " '35',\n",
       " 'ne',\n",
       " 'govorim',\n",
       " 's',\n",
       " 'tobo',\n",
       " '##m',\n",
       " ',',\n",
       " 'druga',\n",
       " '##čija',\n",
       " 'si',\n",
       " ',',\n",
       " 'nisi',\n",
       " 'ista',\n",
       " ',',\n",
       " 'izgubili',\n",
       " 'smo',\n",
       " 'nepov',\n",
       " '##ratn',\n",
       " '##o',\n",
       " ',',\n",
       " 'dje',\n",
       " '##teli',\n",
       " '##nu',\n",
       " 's',\n",
       " 'četiri',\n",
       " 'lista',\n",
       " '.',\n",
       " 'daj',\n",
       " ',',\n",
       " 'pogleda',\n",
       " '##j',\n",
       " 'me',\n",
       " 'u',\n",
       " 'oči',\n",
       " 'što',\n",
       " 'čovjek',\n",
       " 'može',\n",
       " 'da',\n",
       " 'prog',\n",
       " '##uta',\n",
       " ',',\n",
       " 'ima',\n",
       " 'li',\n",
       " 'snage',\n",
       " 'za',\n",
       " 'još',\n",
       " 'pet',\n",
       " 'minuta',\n",
       " '?',\n",
       " 'ili',\n",
       " 'smo',\n",
       " 'stigli',\n",
       " 'do',\n",
       " 'kraja',\n",
       " ',',\n",
       " 'do',\n",
       " 'kraja',\n",
       " 'puta',\n",
       " '?']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_token_class = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "ners = nlp_token_class(text.replace(\"moše pijade 35\", \"[MASK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.16262193024158478,\n",
       "  'token': 1649,\n",
       "  'token_str': 'stvari',\n",
       "  'sequence': 'ne govorim s tobom, i ništa nije kao prije, čija to pisma čitaš? tišina, samo tišina lije. hej, pogledaj me u oči, što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta? ako se rastanemo, onako pošteno i sami, što ćemo reći susjedima, poštaru i tvojoj mami? ako se rastanemo, neće se srušiti svijet, doći će novi stanari, u stvari ne govorim s tobom, drugačija si, nisi ista, izgubili smo nepovratno, djetelinu s četiri lista. daj, pogledaj me u oči što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta?'},\n",
       " {'score': 0.1452377885580063,\n",
       "  'token': 1114,\n",
       "  'token_str': 'biti',\n",
       "  'sequence': 'ne govorim s tobom, i ništa nije kao prije, čija to pisma čitaš? tišina, samo tišina lije. hej, pogledaj me u oči, što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta? ako se rastanemo, onako pošteno i sami, što ćemo reći susjedima, poštaru i tvojoj mami? ako se rastanemo, neće se srušiti svijet, doći će novi stanari, u biti ne govorim s tobom, drugačija si, nisi ista, izgubili smo nepovratno, djetelinu s četiri lista. daj, pogledaj me u oči što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta?'},\n",
       " {'score': 0.06627717614173889,\n",
       "  'token': 1431,\n",
       "  'token_str': 'tome',\n",
       "  'sequence': 'ne govorim s tobom, i ništa nije kao prije, čija to pisma čitaš? tišina, samo tišina lije. hej, pogledaj me u oči, što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta? ako se rastanemo, onako pošteno i sami, što ćemo reći susjedima, poštaru i tvojoj mami? ako se rastanemo, neće se srušiti svijet, doći će novi stanari, u tome ne govorim s tobom, drugačija si, nisi ista, izgubili smo nepovratno, djetelinu s četiri lista. daj, pogledaj me u oči što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta?'},\n",
       " {'score': 0.05677172541618347,\n",
       "  'token': 1019,\n",
       "  'token_str': 'to',\n",
       "  'sequence': 'ne govorim s tobom, i ništa nije kao prije, čija to pisma čitaš? tišina, samo tišina lije. hej, pogledaj me u oči, što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta? ako se rastanemo, onako pošteno i sami, što ćemo reći susjedima, poštaru i tvojoj mami? ako se rastanemo, neće se srušiti svijet, doći će novi stanari, u to ne govorim s tobom, drugačija si, nisi ista, izgubili smo nepovratno, djetelinu s četiri lista. daj, pogledaj me u oči što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta?'},\n",
       " {'score': 0.05350944399833679,\n",
       "  'token': 2716,\n",
       "  'token_str': 'redu',\n",
       "  'sequence': 'ne govorim s tobom, i ništa nije kao prije, čija to pisma čitaš? tišina, samo tišina lije. hej, pogledaj me u oči, što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta? ako se rastanemo, onako pošteno i sami, što ćemo reći susjedima, poštaru i tvojoj mami? ako se rastanemo, neće se srušiti svijet, doći će novi stanari, u redu ne govorim s tobom, drugačija si, nisi ista, izgubili smo nepovratno, djetelinu s četiri lista. daj, pogledaj me u oči što čovjek može da proguta, ima li snage za još pet minuta? ili smo stigli do kraja, do kraja puta?'}]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_token_class = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "ners = nlp_token_class(\"Volim pit i [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3062123954296112,\n",
       "  'token': 11924,\n",
       "  'token_str': 'jesti',\n",
       "  'sequence': 'Volim pit i jesti'},\n",
       " {'score': 0.21018792688846588,\n",
       "  'token': 31844,\n",
       "  'token_str': 'piti',\n",
       "  'sequence': 'Volim pit i piti'},\n",
       " {'score': 0.022669702768325806,\n",
       "  'token': 31456,\n",
       "  'token_str': 'čitati',\n",
       "  'sequence': 'Volim pit i čitati'},\n",
       " {'score': 0.020960301160812378,\n",
       "  'token': 9485,\n",
       "  'token_str': 'uživati',\n",
       "  'sequence': 'Volim pit i uživati'},\n",
       " {'score': 0.014921344816684723,\n",
       "  'token': 2708,\n",
       "  'token_str': 'raditi',\n",
       "  'sequence': 'Volim pit i raditi'}]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(\"EMBEDDIA/crosloengual-bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outext = \"Ja sam ana\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbeBert(nn.Module):\n",
    "  def __init__(self, num_labels):\n",
    "    super().__init__()\n",
    "    self.bert = AutoModel.from_pretrained(\"EMBEDDIA/crosloengual-bert\")\n",
    "    self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    self.to(device)\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.probe.parameters()\n",
    "  \n",
    "  def forward(self, sentences):\n",
    "    with torch.no_grad(): # no training of BERT parameters\n",
    "      word_rep, sentence_rep = self.bert(sentences, return_dict=False)\n",
    "    return self.probe(word_rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
    "label_vocab['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {'<pad>': 0})"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:  15%|█▍        | 60.5M/416M [00:36<01:37, 3.82MB/s]"
     ]
    }
   ],
   "source": [
    "label_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# the model should return a tensor of shape (batch size, sequence length, number of labels)\n",
    "bert_model = LinearProbeBert(len(label_vocab))\n",
    "y = bert_model(torch.tensor([[0, 1, 2], [3, 4, 5]]).to(device))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0753],\n",
       "         [ 0.3247],\n",
       "         [-0.0744]],\n",
       "\n",
       "        [[-0.2319],\n",
       "         [-0.4074],\n",
       "         [-0.4529]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1121d06959d4a1de83cb3123763af6c37fcb19268807b8c73462527e48d9be5d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0b4 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
